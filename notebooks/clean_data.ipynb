{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_PATH = 'datasets'\n",
    "CLEANED_PATH = 'output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv(Path(DATASET_PATH, 'messages.csv'), \n",
    "                       parse_dates=[\n",
    "                            'clicked_first_time_at', \n",
    "                            'clicked_last_time_at',\n",
    "                            'opened_first_time_at',\n",
    "                            'opened_last_time_at', \n",
    "                            'unsubscribed_at',\n",
    "                            'hard_bounced_at',\n",
    "                            'soft_bounced_at',\n",
    "                            'complained_at',\n",
    "                            'purchased_at',\n",
    "                            'blocked_at',\n",
    "                            'created_at',\n",
    "                            'sent_at',\n",
    "                            'updated_at'\n",
    "                        ],\n",
    "                        date_format='%Y-%m-%d %H:%M:%S.%f',\n",
    "                        true_values=['t'], false_values=['f'],\n",
    "                        dtype={\n",
    "                            'message_id':'string',\n",
    "                            'campaign_id':'int32',\n",
    "                            'message_type':'category',\n",
    "                            'channel':'category',\n",
    "                            'platform':'category', \n",
    "                            'email_provider':'string', \n",
    "                            'user_device_id':'int16', \n",
    "                            'user_id':'int32'                           \n",
    "                        }\n",
    "                        ).drop(columns=['category','stream','id','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['message_id'] = messages['message_id'].str.replace(\n",
    "    r'(\\w{8})(\\w{4})(\\w{4})(\\w{3})-(\\w)(\\w{12})', \n",
    "    r'\\1-\\2-\\3-\\4\\5-\\6', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_messages = messages[['campaign_id', 'message_type']].drop_duplicates().reset_index(drop=True)\n",
    "unique_messages['id'] = unique_messages.index + 1\n",
    "messages = messages.merge(unique_messages, on=['campaign_id', 'message_type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = messages[['client_id','user_id','user_device_id']].drop_duplicates()\n",
    "users = messages['user_id'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_cols = [col.replace('is_', '') \n",
    "                      for col in messages.columns \n",
    "                      if col.startswith('is_')]\n",
    "\n",
    "message_behavior = [ \n",
    "    (\n",
    "        messages[['message_id', f'is_{b}']].rename(columns={f'is_{b}': 'flag'})\n",
    "        .assign(\n",
    "            type = b,\n",
    "            happened_first_time = messages.get(f'{b}_first_time_at', messages.get(f'{b}_at', pd.NaT)),\n",
    "            happened_last_time = messages.get(f'{b}_last_time_at', pd.NaT)\n",
    "        ))\n",
    "    for b in behaviors_cols\n",
    "]\n",
    "\n",
    "message_behavior = pd.concat(message_behavior, ignore_index=True)\n",
    "\n",
    "message_behavior = message_behavior[message_behavior['flag'] == True]\\\n",
    "    .drop(columns=['flag'])\\\n",
    "    .sort_values(['message_id','happened_first_time'])\\\n",
    "    .set_index(['message_id','type'])\n",
    "\n",
    "message_behavior.to_csv(Path(CLEANED_PATH, 'message_behavior.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_grouped = message_behavior.iloc[:5000].reset_index().groupby('message_id')\\\n",
    "    [['message_id','type','happened_first_time','happened_last_time']].apply(\n",
    "    lambda grp: grp[['type','happened_first_time', 'happened_last_time']].to_dict('records')\n",
    ").reset_index(name='behaviors')\n",
    "del(message_behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_grouped.to_json('behavior_groups.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_sent = messages[['message_id','id','client_id',\n",
    "                         'email_provider','platform',\n",
    "                         'created_at','updated_at', 'sent_at']].set_index('message_id')\n",
    "message_sent.to_csv(Path(CLEANED_PATH, 'message_sent.csv'))\n",
    "del(message_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = messages[['id','campaign_id','message_type','channel']].drop_duplicates(['campaign_id','message_type']).set_index('id')\n",
    "messages.to_csv(Path(CLEANED_PATH,'messages.csv'))\n",
    "del(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1907, 19)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campaigns = pd.read_csv('datasets/campaigns.csv',\n",
    "                        parse_dates=[\n",
    "                            'started_at', \n",
    "                            'finished_at'\n",
    "                        ],\n",
    "                        dtype={\n",
    "                            'campaign_type':'category',\n",
    "                            'channel':'category',\n",
    "                            'topic':'string',\n",
    "                            'total_count':'Int32',\n",
    "                            'ab_test':'boolean',\n",
    "                            'warmup_mode':'boolean',\n",
    "                            'hour_limit':'Int32',\n",
    "                            'subject_length':'Int16',\n",
    "                            'subject_with_personalization':'boolean',\n",
    "                            'subject_with_deadline':'boolean',\n",
    "                            'subject_with_emoji':'boolean',\n",
    "                            'subject_with_bonuses':'boolean',\n",
    "                            'subject_with_discount':'boolean',\n",
    "                            'subject_with_saleout':'boolean',\n",
    "                            'is_test':'boolean',\n",
    "                            'position':'Int16' \n",
    "                        }\n",
    "                        )\\\n",
    "                        .fillna({'ab_test':False,\n",
    "                                 'warmup_mode':False,\n",
    "                                 'is_test':False})\\\n",
    "                        \n",
    "campaigns.index.names = ['campaign_pk']\n",
    "campaigns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1892, 18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campaigns = campaigns[~(\n",
    "    (campaigns['is_test'] == True)\n",
    "    ### Bulk rules\n",
    "    | (campaigns['campaign_type'] == 'bulk') \n",
    "        & (campaigns['started_at'].isna() \n",
    "           | (campaigns['warmup_mode'] == True) & campaigns['hour_limit'].isna())\n",
    "    ### Trigger rules\n",
    "    | (campaigns['campaign_type'] == 'trigger')\n",
    "        & campaigns['position'].isna() \n",
    "    )].drop(columns=['is_test'])\n",
    "campaigns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>campaign_type</th>\n",
       "      <th>channel</th>\n",
       "      <th>topic</th>\n",
       "      <th>started_at</th>\n",
       "      <th>finished_at</th>\n",
       "      <th>total_count</th>\n",
       "      <th>ab_test</th>\n",
       "      <th>warmup_mode</th>\n",
       "      <th>hour_limit</th>\n",
       "      <th>subject_length</th>\n",
       "      <th>subject_with_personalization</th>\n",
       "      <th>subject_with_deadline</th>\n",
       "      <th>subject_with_emoji</th>\n",
       "      <th>subject_with_bonuses</th>\n",
       "      <th>subject_with_discount</th>\n",
       "      <th>subject_with_saleout</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign_pk</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>20519</td>\n",
       "      <td>trigger</td>\n",
       "      <td>multichannel</td>\n",
       "      <td>abandoned cart</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>20896</td>\n",
       "      <td>trigger</td>\n",
       "      <td>multichannel</td>\n",
       "      <td>abandoned cart</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>21379</td>\n",
       "      <td>trigger</td>\n",
       "      <td>multichannel</td>\n",
       "      <td>abandoned cart</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id campaign_type       channel           topic started_at  \\\n",
       "campaign_pk                                                                 \n",
       "1830         20519       trigger  multichannel  abandoned cart        NaT   \n",
       "1836         20896       trigger  multichannel  abandoned cart        NaT   \n",
       "1837         21379       trigger  multichannel  abandoned cart        NaT   \n",
       "\n",
       "            finished_at  total_count  ab_test  warmup_mode  hour_limit  \\\n",
       "campaign_pk                                                              \n",
       "1830                NaT         <NA>    False        False        <NA>   \n",
       "1836                NaT         <NA>    False        False        <NA>   \n",
       "1837                NaT         <NA>    False        False        <NA>   \n",
       "\n",
       "             subject_length  subject_with_personalization  \\\n",
       "campaign_pk                                                 \n",
       "1830                   <NA>                          <NA>   \n",
       "1836                   <NA>                          <NA>   \n",
       "1837                   <NA>                          <NA>   \n",
       "\n",
       "             subject_with_deadline  subject_with_emoji  subject_with_bonuses  \\\n",
       "campaign_pk                                                                    \n",
       "1830                          <NA>                <NA>                  <NA>   \n",
       "1836                          <NA>                <NA>                  <NA>   \n",
       "1837                          <NA>                <NA>                  <NA>   \n",
       "\n",
       "             subject_with_discount  subject_with_saleout  position  \n",
       "campaign_pk                                                         \n",
       "1830                          <NA>                  <NA>         0  \n",
       "1836                          <NA>                  <NA>         4  \n",
       "1837                          <NA>                  <NA>         4  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (From description): Multichannel is a kind of transactional campaign when a retailer \n",
    "# tries to deliver important information from the cheapest channel \n",
    "# to the more expensive, depending on which channels recipient uses.\n",
    "campaigns[campaigns['channel']=='multichannel'].head(3)\n",
    "# But these multichannel campaign subset related only to trigger campaign_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "channel\n",
       "mobile_push     1394\n",
       "email            479\n",
       "sms                1\n",
       "multichannel       0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (From description): SMS doesn’t have subject\n",
    "campaigns[campaigns['subject_length'].notna()]['channel'].value_counts(dropna=False)\n",
    "# But actually sms has subject\n",
    "# I'll drop subject for sms because it doesn't meet business requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of compaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### MONGODB ######################\n",
    "# Build embedded subdocuments for campaign details.\n",
    "def build_campaign_doc(row):\n",
    "    doc = {\n",
    "        \"id\": row[\"id\"],\n",
    "        \"campaign_type\": row[\"campaign_type\"],\n",
    "        \"channel\": row[\"channel\"],\n",
    "        \"topic\": row.get(\"topic\")\n",
    "    }\n",
    "    if row[\"campaign_type\"] == \"bulk\":\n",
    "        doc[\"bulk_details\"] = {\n",
    "            \"started_at\": row[\"started_at\"],\n",
    "            \"finished_at\": row.get(\"finished_at\"),\n",
    "            \"total_count\": row.get(\"total_count\", None),\n",
    "            \"warmup_mode\": row[\"warmup_mode\"],\n",
    "            \"hour_limit\": row.get(\"hour_limit\", None),\n",
    "            \"ab_test\": row[\"ab_test\"]\n",
    "        }\n",
    "    if row[\"campaign_type\"] == \"trigger\":\n",
    "        doc[\"trigger_details\"] = {\"position\": row.get(\"position\", None)}\n",
    "    if row[\"channel\"] not in [\"sms\", \"multichannel\"]:\n",
    "        doc[\"subject_details\"] = {\n",
    "            \"subject_length\": row.get(\"subject_length\", None),\n",
    "            \"subject_with_personalization\": row[\"subject_with_personalization\"],\n",
    "            \"subject_with_deadline\": row[\"subject_with_deadline\"],\n",
    "            \"subject_with_emoji\": row[\"subject_with_emoji\"],\n",
    "            \"subject_with_bonuses\": row[\"subject_with_bonuses\"],\n",
    "            \"subject_with_discount\": row[\"subject_with_discount\"],\n",
    "            \"subject_with_saleout\": row[\"subject_with_saleout\"]\n",
    "        }\n",
    "    return doc\n",
    "\n",
    "campaigns_docs = campaigns.reset_index().apply(build_campaign_doc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_cols = [\n",
    "        # Only bulk campaigns have start date and finish date\n",
    "        'started_at', \n",
    "        'finished_at', # Finish date is the date/time when campaign did sent the *final* message -> can be NaN\n",
    "        'total_count', # Says how many recipients was in bulk campaign when it was sent.\n",
    "        'warmup_mode', #  Campaign works days or weeks\n",
    "        'hour_limit', # with rule: how many messages can be sent every hour\n",
    "        'ab_test' #  A/B test mode (campaign to a limited audience)\n",
    "    ]\n",
    "bulks = campaigns[campaigns['campaign_type']=='bulk'][bulk_cols]\n",
    "bulks.to_csv(Path(CLEANED_PATH, 'bulks.csv'))\n",
    "del(bulks)\n",
    "\n",
    "subject_cols = [\n",
    "        'subject_length',\n",
    "        'subject_with_personalization',\n",
    "        'subject_with_deadline',\n",
    "        'subject_with_emoji',\n",
    "        'subject_with_bonuses',\n",
    "        'subject_with_discount',\n",
    "        'subject_with_saleout'\n",
    "    ]\n",
    "campaign_subject = campaigns[~campaigns['channel']\\\n",
    "                             .isin(['sms', 'multichannel'])][subject_cols]\n",
    "campaign_subject.to_csv(Path(CLEANED_PATH, 'campaign_subject.csv'))\n",
    "del(campaign_subject)\n",
    "\n",
    "trigger_cols = ['position']\n",
    "triggers = campaigns[campaigns['campaign_type']=='trigger'][trigger_cols]\n",
    "triggers.to_csv(Path(CLEANED_PATH, 'triggers.csv'))\n",
    "del(triggers)\n",
    "\n",
    "campaigns = campaigns.drop(columns=bulk_cols \n",
    "                                + subject_cols \n",
    "                                + trigger_cols)\n",
    "campaigns.to_csv(Path(CLEANED_PATH, 'campaigns.csv'))\n",
    "del(campaigns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv('datasets/events.csv',\n",
    "                     parse_dates=['event_time'],\n",
    "                     date_format='%Y-%m-%d %H:%M:%S UTC',\n",
    "                     dtype={\n",
    "                         'event_type':'category',\n",
    "                         'product_id':'int32',\n",
    "                         'category_id':'int64',\n",
    "                         'category_code':'category',\n",
    "                         'brand':'category',\n",
    "                         'price':'float32',\n",
    "                         'user_id':'int32',\n",
    "                         'user_session':'string'\n",
    "                     }\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.concat([users, \n",
    "                   events['user_id'].drop_duplicates()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose the representative value from category_code within each group.\n",
    "def choose_representative(series):\n",
    "    non_null = series.dropna()\n",
    "    if non_null.empty: return np.nan\n",
    "    else:\n",
    "        mode_vals = non_null.mode()\n",
    "        if not mode_vals.empty:\n",
    "            return mode_vals.iloc[0]\n",
    "        else:\n",
    "            return non_null.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_products = events.groupby(['product_id', 'category_id'], as_index=False)['category_code']\\\n",
    "    .agg(choose_representative)\n",
    "unique_products['product_pk'] = unique_products.index + 1\n",
    "events = events.drop(columns='category_code').merge(unique_products, on=['product_id', 'category_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_produc_cards = events[['product_pk', 'brand']].drop_duplicates().reset_index(drop=True)\n",
    "unique_produc_cards['product_card_pk'] = unique_produc_cards.index + 1\n",
    "events = events.merge(unique_produc_cards, on=['product_pk', 'brand'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = events[['product_pk','product_id', 'category_id', 'category_code']].drop_duplicates().set_index('product_pk')\n",
    "products.to_csv(Path(CLEANED_PATH, 'products.csv'))\n",
    "del(products)\n",
    "product_cards = events[['product_card_pk','product_pk','brand']].drop_duplicates().set_index('product_card_pk')\n",
    "product_cards.to_csv(Path(CLEANED_PATH, 'product_cards.csv'))\n",
    "del(product_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events.drop_duplicates(['product_card_pk','user_id','event_time'])\\\n",
    "                [['product_card_pk','user_id','event_time',\n",
    "                 'event_type','user_session','price']]\n",
    "events.to_csv(Path(CLEANED_PATH, 'events.csv'))\n",
    "del(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client first purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_purchase = pd.read_csv('datasets/client_first_purchase_date.csv',\n",
    "                             parse_dates=['first_purchase_date'],\n",
    "                             date_format='%Y-%m-%d',\n",
    "                             dtype={'user_id':'int32',\n",
    "                             'user_device_id':'int16'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = pd.concat([clients,\n",
    "                     first_purchase[['client_id',\n",
    "                                     'user_id',\n",
    "                                     'user_device_id']].drop_duplicates()])\n",
    "clients.to_csv(Path(CLEANED_PATH, 'clients.csv'))\n",
    "users = pd.concat([users, \n",
    "                   first_purchase['user_id'].drop_duplicates()])\n",
    "users.to_csv(Path(CLEANED_PATH, 'users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_purchase = first_purchase[['client_id',\n",
    "                                 'first_purchase_date']].drop_duplicates()\n",
    "first_purchase.to_csv(Path(CLEANED_PATH, 'first_purchase.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = pd.read_csv('datasets/friends.csv',\n",
    "                        dtype={\n",
    "                            'friend1':'int32',\n",
    "                            'friend2':'int32'\n",
    "                        })\n",
    "friends = pd.DataFrame(np.sort(friends.values, axis=1), columns=friends.columns)\n",
    "users = pd.concat([users, \n",
    "                   friends['friend1'].drop_duplicates()]).drop_duplicates()\n",
    "users = pd.concat([users, \n",
    "                   friends['friend2'].drop_duplicates()]).drop_duplicates()\n",
    "friends.to_csv(Path(CLEANED_PATH, 'friends.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.merge(users.rename('user_id'), clients, on='user_id', how='left')\n",
    "users = users.merge(first_purchase[['client_id','first_purchase_date']], how='left', on='client_id')\n",
    "\n",
    "grouped = users.iloc[:5000].groupby('user_id')[['user_id', 'client_id', 'user_device_id', 'first_purchase_date']].apply(\n",
    "    lambda x: \n",
    "    x[['client_id', 'user_device_id', 'first_purchase_date']].to_dict('records')).reset_index()\n",
    "grouped.columns = ['user_id', 'devices']\n",
    "\n",
    "def remove_empty_dates(device_list):\n",
    "    return [\n",
    "        {k: v for k, v in device.items() if pd.notna(v)}\n",
    "        for device in device_list\n",
    "    ]\n",
    "\n",
    "# Применение функции к колонке 'devices'\n",
    "grouped['devices'] = grouped['devices'].apply(remove_empty_dates)\n",
    "grouped.to_json('grouped_users.json', orient='records', date_format='iso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(users)\n",
    "del(first_purchase)\n",
    "del(clients)\n",
    "del(friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос успешен!\n",
      "{'candidates': [{'content': {'parts': [{'text': 'The curse of dimensionality refers to the various challenges and problems that arise when dealing with high-dimensional data, i.e., data with a large number of features or attributes (dimensions).  It essentially means that as the number of dimensions grows, the data becomes increasingly sparse, leading to a number of detrimental effects on machine learning algorithms and data analysis techniques.\\n\\nHere\\'s a breakdown of the key aspects of the curse of dimensionality:\\n\\n**1. Data Sparsity:**\\n\\n*   **The Problem:** Imagine a square (2D) where you have a few data points scattered.  It\\'s relatively easy to find points that are \"close\" to each other. Now, imagine a cube (3D) with the same number of data points.  The points are now more spread out, and it\\'s harder to find neighbors.  This effect intensifies dramatically as you add more dimensions.  The data becomes extremely sparse, meaning there are vast regions with no data points.\\n*   **Why it\\'s a curse:** Many machine learning algorithms rely on finding similar data points (neighbors) for tasks like classification, regression, and clustering.  If the data is sparse, these algorithms struggle to find reliable neighbors, leading to poor performance.  Distances become less meaningful.  Almost all points are far from each other.\\n\\n**2. Increased Computational Complexity:**\\n\\n*   **The Problem:**  Many algorithms have a time complexity that increases exponentially with the number of dimensions. For example, some search algorithms need to explore all possible combinations of features, which becomes computationally intractable in high dimensions.  Calculating distances between points becomes more expensive.\\n*   **Why it\\'s a curse:**  Training models and making predictions can become incredibly slow and resource-intensive, rendering some algorithms unusable.\\n\\n**3. Overfitting:**\\n\\n*   **The Problem:** In high-dimensional spaces, it\\'s easier to find patterns (even spurious ones) in the training data that don\\'t generalize well to unseen data.  The model essentially memorizes the training data instead of learning underlying relationships. With a sufficient number of features, even random noise can appear to be a meaningful pattern.\\n*   **Why it\\'s a curse:**  Overfitting leads to high accuracy on the training data but poor performance on new, unseen data. The model is too specific to the training set and doesn\\'t generalize well.\\n\\n**4. The Concentration of Distances:**\\n\\n*   **The Problem:** In high dimensions, the distances between different data points tend to become very similar.  The difference between the nearest and furthest neighbors shrinks. This phenomenon is called \"distance concentration.\"\\n*   **Why it\\'s a curse:**  It makes it difficult to distinguish between data points based on distance. Algorithms that rely on distance metrics (like k-Nearest Neighbors) become less effective.  The concept of \"neighborhood\" becomes less useful.\\n\\n**5. Feature Selection Challenges:**\\n\\n*   **The Problem:** In high-dimensional spaces, it becomes challenging to identify the most relevant features.  With many features, it\\'s difficult to determine which ones are actually important for the task at hand and which ones are just noise. Exhaustive search for the best subset of features is often computationally impossible.\\n*   **Why it\\'s a curse:**  Irrelevant or redundant features can degrade performance.  Finding the optimal subset of features is a complex optimization problem.\\n\\n**Examples of the Curse in Action:**\\n\\n*   **Image Recognition:**  Consider classifying images of cats and dogs.  Each pixel in the image can be considered a feature.  High-resolution images have a very high number of pixels (dimensions).  The curse of dimensionality can make it difficult to train a classifier that generalizes well, as the data becomes sparse and overfitting is more likely.\\n*   **Genomics:**  Analyzing gene expression data, where each gene represents a feature.  With tens of thousands of genes, the curse of dimensionality can be a significant challenge.\\n*   **Natural Language Processing:**  Representing text documents using a \"bag of words\" approach, where each unique word in the vocabulary is a feature.  The vocabulary size can be very large, leading to a high-dimensional representation.\\n\\n**How to Mitigate the Curse:**\\n\\nThere are several techniques to mitigate the curse of dimensionality:\\n\\n*   **Dimensionality Reduction:**\\n    *   **Feature Selection:** Choose a subset of the most relevant features. Methods include filtering (e.g., based on variance or correlation), wrapper methods (e.g., using a search algorithm to find the best subset), and embedded methods (e.g., using regularized models that automatically select features).\\n    *   **Feature Extraction:** Transform the data into a lower-dimensional space while preserving important information. Techniques include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), t-distributed Stochastic Neighbor Embedding (t-SNE), and autoencoders.\\n*   **Regularization:**  Add penalties to the model to prevent overfitting.  Examples include L1 (Lasso) and L2 (Ridge) regularization.\\n*   **Increasing the Sample Size:**  Collecting more data can help to alleviate the sparsity problem. However, this may not always be feasible or cost-effective.\\n*   **Using Algorithms Robust to High Dimensionality:**  Some algorithms are less susceptible to the curse of dimensionality than others.  For example, decision trees can perform relatively well in high-dimensional spaces because they perform feature selection as part of the learning process. Ensemble methods like random forests can also be effective.\\n*   **Data Cleaning and Preprocessing:** Remove noise and outliers from the data, which can be amplified in high dimensions.\\n*   **Feature Engineering:** Create new features that are more informative and less redundant than the original features.\\n\\nIn summary, the curse of dimensionality is a significant challenge in machine learning and data analysis.  It\\'s crucial to be aware of its potential effects and to employ appropriate techniques to mitigate its impact. Selecting the right approach depends on the specific dataset, algorithm, and task at hand.\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'citationMetadata': {'citationSources': [{'endIndex': 124, 'uri': 'https://github.com/Pramod1818/Assignment'}, {'startIndex': 4844, 'endIndex': 4992, 'uri': 'https://blab.com/blab?q=Dimensionality%20Reduction%20Techniques'}]}, 'avgLogprobs': -0.3435329834178291}], 'usageMetadata': {'promptTokenCount': 7, 'candidatesTokenCount': 1259, 'totalTokenCount': 1266, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 7}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 1259}]}, 'modelVersion': 'gemini-2.0-flash'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "api_key = \"AIzaSyBMpS3Ki6YGXbi0cFwXKehmxlqCOlA2oXc\"\n",
    "\n",
    "url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}\"\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "payload = {\n",
    "  \"contents\": [{\n",
    "    \"parts\":[{\"text\": \"Explain the curse of dimentionality\"}]\n",
    "    }]\n",
    "   }\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.status_code == 200:\n",
    "    print(\"Запрос успешен!\")\n",
    "    print(response.json()) # Выводит JSON ответ от сервера\n",
    "else:\n",
    "    print(f\"Ошибка запроса: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidates': [{'content': {'parts': [{'text': 'The curse of dimensionality refers to the various challenges and problems that arise when dealing with high-dimensional data, i.e., data with a large number of features or attributes (dimensions).  It essentially means that as the number of dimensions grows, the data becomes increasingly sparse, leading to a number of detrimental effects on machine learning algorithms and data analysis techniques.\\n\\nHere\\'s a breakdown of the key aspects of the curse of dimensionality:\\n\\n**1. Data Sparsity:**\\n\\n*   **The Problem:** Imagine a square (2D) where you have a few data points scattered.  It\\'s relatively easy to find points that are \"close\" to each other. Now, imagine a cube (3D) with the same number of data points.  The points are now more spread out, and it\\'s harder to find neighbors.  This effect intensifies dramatically as you add more dimensions.  The data becomes extremely sparse, meaning there are vast regions with no data points.\\n*   **Why it\\'s a curse:** Many machine learning algorithms rely on finding similar data points (neighbors) for tasks like classification, regression, and clustering.  If the data is sparse, these algorithms struggle to find reliable neighbors, leading to poor performance.  Distances become less meaningful.  Almost all points are far from each other.\\n\\n**2. Increased Computational Complexity:**\\n\\n*   **The Problem:**  Many algorithms have a time complexity that increases exponentially with the number of dimensions. For example, some search algorithms need to explore all possible combinations of features, which becomes computationally intractable in high dimensions.  Calculating distances between points becomes more expensive.\\n*   **Why it\\'s a curse:**  Training models and making predictions can become incredibly slow and resource-intensive, rendering some algorithms unusable.\\n\\n**3. Overfitting:**\\n\\n*   **The Problem:** In high-dimensional spaces, it\\'s easier to find patterns (even spurious ones) in the training data that don\\'t generalize well to unseen data.  The model essentially memorizes the training data instead of learning underlying relationships. With a sufficient number of features, even random noise can appear to be a meaningful pattern.\\n*   **Why it\\'s a curse:**  Overfitting leads to high accuracy on the training data but poor performance on new, unseen data. The model is too specific to the training set and doesn\\'t generalize well.\\n\\n**4. The Concentration of Distances:**\\n\\n*   **The Problem:** In high dimensions, the distances between different data points tend to become very similar.  The difference between the nearest and furthest neighbors shrinks. This phenomenon is called \"distance concentration.\"\\n*   **Why it\\'s a curse:**  It makes it difficult to distinguish between data points based on distance. Algorithms that rely on distance metrics (like k-Nearest Neighbors) become less effective.  The concept of \"neighborhood\" becomes less useful.\\n\\n**5. Feature Selection Challenges:**\\n\\n*   **The Problem:** In high-dimensional spaces, it becomes challenging to identify the most relevant features.  With many features, it\\'s difficult to determine which ones are actually important for the task at hand and which ones are just noise. Exhaustive search for the best subset of features is often computationally impossible.\\n*   **Why it\\'s a curse:**  Irrelevant or redundant features can degrade performance.  Finding the optimal subset of features is a complex optimization problem.\\n\\n**Examples of the Curse in Action:**\\n\\n*   **Image Recognition:**  Consider classifying images of cats and dogs.  Each pixel in the image can be considered a feature.  High-resolution images have a very high number of pixels (dimensions).  The curse of dimensionality can make it difficult to train a classifier that generalizes well, as the data becomes sparse and overfitting is more likely.\\n*   **Genomics:**  Analyzing gene expression data, where each gene represents a feature.  With tens of thousands of genes, the curse of dimensionality can be a significant challenge.\\n*   **Natural Language Processing:**  Representing text documents using a \"bag of words\" approach, where each unique word in the vocabulary is a feature.  The vocabulary size can be very large, leading to a high-dimensional representation.\\n\\n**How to Mitigate the Curse:**\\n\\nThere are several techniques to mitigate the curse of dimensionality:\\n\\n*   **Dimensionality Reduction:**\\n    *   **Feature Selection:** Choose a subset of the most relevant features. Methods include filtering (e.g., based on variance or correlation), wrapper methods (e.g., using a search algorithm to find the best subset), and embedded methods (e.g., using regularized models that automatically select features).\\n    *   **Feature Extraction:** Transform the data into a lower-dimensional space while preserving important information. Techniques include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), t-distributed Stochastic Neighbor Embedding (t-SNE), and autoencoders.\\n*   **Regularization:**  Add penalties to the model to prevent overfitting.  Examples include L1 (Lasso) and L2 (Ridge) regularization.\\n*   **Increasing the Sample Size:**  Collecting more data can help to alleviate the sparsity problem. However, this may not always be feasible or cost-effective.\\n*   **Using Algorithms Robust to High Dimensionality:**  Some algorithms are less susceptible to the curse of dimensionality than others.  For example, decision trees can perform relatively well in high-dimensional spaces because they perform feature selection as part of the learning process. Ensemble methods like random forests can also be effective.\\n*   **Data Cleaning and Preprocessing:** Remove noise and outliers from the data, which can be amplified in high dimensions.\\n*   **Feature Engineering:** Create new features that are more informative and less redundant than the original features.\\n\\nIn summary, the curse of dimensionality is a significant challenge in machine learning and data analysis.  It\\'s crucial to be aware of its potential effects and to employ appropriate techniques to mitigate its impact. Selecting the right approach depends on the specific dataset, algorithm, and task at hand.\\n'}],\n",
       "    'role': 'model'},\n",
       "   'finishReason': 'STOP',\n",
       "   'citationMetadata': {'citationSources': [{'endIndex': 124,\n",
       "      'uri': 'https://github.com/Pramod1818/Assignment'},\n",
       "     {'startIndex': 4844,\n",
       "      'endIndex': 4992,\n",
       "      'uri': 'https://blab.com/blab?q=Dimensionality%20Reduction%20Techniques'}]},\n",
       "   'avgLogprobs': -0.3435329834178291}],\n",
       " 'usageMetadata': {'promptTokenCount': 7,\n",
       "  'candidatesTokenCount': 1259,\n",
       "  'totalTokenCount': 1266,\n",
       "  'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 7}],\n",
       "  'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 1259}]},\n",
       " 'modelVersion': 'gemini-2.0-flash'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
